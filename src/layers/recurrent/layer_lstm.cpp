/*
* EDDL Library - European Distributed Deep Learning Library.
* Version: 0.9
* copyright (c) 2020, Universidad Polit√©cnica de Valencia (UPV), PRHLT Research Centre
* Date: November 2020
* Author: PRHLT Research Centre, UPV, (rparedes@prhlt.upv.es), (jon@prhlt.upv.es)
* All rights reserved
*/



#include <cstdio>
#include <cstdio>
#include <cstdio>
#include <cstdlib>
#include <iostream>

#include "eddl/layers/recurrent/layer_recurrent.h"


using namespace std;

int LLSTM::total_layers = 0;

LLSTM::LLSTM(vector<Layer *> parent, int units, bool mask_zeros, bool bidirectional, string name, int dev, int mem): MLayer(name, dev, mem) {

    this->units = units;
    this->bidirectional = bidirectional;
    this->mask_zeros=mask_zeros;

    isrecurrent=true;

    if (parent[0]->output->ndim != 2) msg("LLSTM only works over 2D tensors", "LLSTM");

    if(name.empty()) this->name = "LSTM" + to_string(++total_layers);

    input = parent[0]->output;
    state_h = output = new Tensor(vector<int>{input->shape[0], units}, dev);
    state_c = new Tensor(vector<int>{input->shape[0], units}, dev);

    states.push_back(state_h);
    states.push_back(state_c);


    Wix = new Tensor(vector<int>{input->shape[1], units}, dev);
    Wfx = new Tensor(vector<int>{input->shape[1], units}, dev);
    Wox = new Tensor(vector<int>{input->shape[1], units}, dev);
    Wcx = new Tensor(vector<int>{input->shape[1], units}, dev);
    params.push_back(Wix);
    params.push_back(Wfx);
    params.push_back(Wox);
    params.push_back(Wcx);

    gWix = new Tensor(vector<int>{input->shape[1], units}, dev);
    gWfx = new Tensor(vector<int>{input->shape[1], units}, dev);
    gWox = new Tensor(vector<int>{input->shape[1], units}, dev);
    gWcx = new Tensor(vector<int>{input->shape[1], units}, dev);
    gradients.push_back(gWix);
    gradients.push_back(gWfx);
    gradients.push_back(gWox);
    gradients.push_back(gWcx);

    Wih = new Tensor(vector<int>{units, units}, dev);
    Wfh = new Tensor(vector<int>{units, units}, dev);
    Woh = new Tensor(vector<int>{units, units}, dev);
    Wch = new Tensor(vector<int>{units, units}, dev);
    params.push_back(Wih);
    params.push_back(Wfh);
    params.push_back(Woh);
    params.push_back(Wch);

    gWih = new Tensor(vector<int>{units, units}, dev);
    gWfh = new Tensor(vector<int>{units, units}, dev);
    gWoh = new Tensor(vector<int>{units, units}, dev);
    gWch = new Tensor(vector<int>{units, units}, dev);
    gradients.push_back(gWih);
    gradients.push_back(gWfh);
    gradients.push_back(gWoh);
    gradients.push_back(gWch);

    inbias = new Tensor(vector<int>{units}, dev);
    params.push_back(inbias);
    ginbias = new Tensor(vector<int>{units}, dev);
    gradients.push_back(ginbias);

    fnbias = new Tensor(vector<int>{units}, dev);
    params.push_back(fnbias);
    gfnbias = new Tensor(vector<int>{units}, dev);
    gradients.push_back(gfnbias);

    onbias = new Tensor(vector<int>{units}, dev);
    params.push_back(onbias);
    gonbias = new Tensor(vector<int>{units}, dev);
    gradients.push_back(gonbias);

    cnbias = new Tensor(vector<int>{units}, dev);
    params.push_back(cnbias);
    gcnbias = new Tensor(vector<int>{units}, dev);
    gradients.push_back(gcnbias);

    for (int i = 0; i < parent.size(); ++i) {
        parent[i]->addchild(this);
        addparent(parent[i]);
    }



}

LLSTM::~LLSTM(){
    // delete state_c; -- not required here, it is deleted in the destructor of the root parent class Layer
}

// RESIZE , MEM_DELTA states
void LLSTM::mem_delta(){
    // Reserve space for delta
    if(delta == nullptr){

//        // Delete deltas
//        for (int i = 0; i < delta_states.size(); ++i) { delete delta_states[i]; delta_states[i] = nullptr;}
        delta_states.clear();
        delta_h = delta = Tensor::zeros(this->output->shape, this->output->device);
        delta_c = Tensor::zeros(this->output->shape, this->output->device);

        delta_states.push_back(delta_h);
        delta_states.push_back(delta_c);


        if(this->verbosity_level >= 2){
            std::cout << "Booked delta for: " + this->name << std::endl;
        }
    }
}

void LLSTM::free_delta(){
    if (this->delta != nullptr){
        // The Tensor destructor takes into account the device details
        delete this->delta;
        this->delta = nullptr;  // Ensure nullptr

        delete this->delta_c;
        this->delta_c = nullptr;

        delta_states.clear();

        if(this->verbosity_level >= 2){
            std::cout << "Deleted delta for: " + this->name << std::endl;
        }
    }
}


void LLSTM::resize(int batch){
    if (output!=nullptr) {
        output->resize(batch);
        state_c->resize(batch);
    }

}

// {nxd} --> {nx1}
void reduced_abs_sum(Tensor * input, Tensor *output)
{
    Tensor *A=input->clone();
    A->abs_();

    Tensor *ones=new Tensor({input->shape[1],1},input->device);
    ones->fill_(1.0);

    // {nxd} x {dx1} --> {nx1}
    Tensor::mult2D(A,0,ones,0,output,0);

    delete A;
    delete ones;

}

// {nx1} --> {nxd}
Tensor *replicate_tensor(Tensor *input,int d)
{

    Tensor *ones=new Tensor({1,d},input->device);
    ones->fill_(1.0);

    Tensor *output=new Tensor({input->shape[0],d},input->device);

    // {nx1} x {1xd} --> {nxd}
    Tensor::mult2D(input,0,ones,0,output,0);

    delete ones;

    return output;
}


// virtual
void LLSTM::forward() {
    if (mask_zeros) {
        mask=new Tensor({input->shape[0],1},dev);
        reduced_abs_sum(input,mask);

        Tensor::logical_not(mask,mask);
        if (parent.size()>1) {
            Tensor *A=replicate_tensor(mask,units);

            psh=parent[1]->states[0]->clone(); //prev state_h
            psc=parent[1]->states[1]->clone(); //prev state_c

            Tensor::el_mult(A,psh,psh,0);
            Tensor::el_mult(A,psc,psc,0);
            delete A;
        }

    }


    // input=parent[0]->output
    in=new Tensor({input->shape[0], units}, dev);

    Tensor::mult2D(parent[0]->output, 0, Wix, 0, in, 0);
    if (parent.size()>1) {
        Tensor::mult2D(parent[1]->states[0], 0, Wih, 0, in, 1);
    }
    Tensor::sum2D_rowwise(in, inbias, in);
    tensorNN::Sigmoid(in, in);

    fn=new Tensor({input->shape[0], units}, dev);
    Tensor::mult2D(parent[0]->output, 0, Wfx, 0, fn, 0);
    if (parent.size()>1) {
        Tensor::mult2D(parent[1]->states[0], 0, Wfh, 0, fn, 1);
    }
    Tensor::sum2D_rowwise(fn, fnbias, fn);
    tensorNN::Sigmoid(fn, fn);

    on=new Tensor({input->shape[0], units}, dev);
    Tensor::mult2D(parent[0]->output, 0, Wox, 0, on, 0);
    if (parent.size()>1) {
        Tensor::mult2D(parent[1]->states[0], 0, Woh, 0, on, 1);
    }
    Tensor::sum2D_rowwise(on, onbias, on);
    tensorNN::Sigmoid(on, on);

    cn=new Tensor({input->shape[0], units}, dev);
    Tensor::mult2D(parent[0]->output, 0, Wcx, 0, cn, 0);
    if (parent.size()>1) {
        Tensor::mult2D(parent[1]->states[0], 0, Wch, 0, cn, 1);
    }
    Tensor::sum2D_rowwise(cn, cnbias, cn);
    tensorNN::Tanh(cn,cn);

    incn=new Tensor({input->shape[0], units}, dev);
    Tensor::el_mult(in,cn,incn,0);


    cn1fn=new Tensor({input->shape[0], units}, dev);
    if (parent.size()>1) {
        Tensor::el_mult(parent[1]->states[1],fn,cn1fn,0);
    }
    else {
        cn1fn->fill_(0.0);
    }

    Tensor::add(1.0,incn,1.0,cn1fn,state_c,0);

    sh=new Tensor({input->shape[0], units}, dev);
    tensorNN::Tanh(state_c,sh);
    //ReLu(state_c,sh);

    Tensor::el_mult(sh,on,state_h,0);

    if (mask_zeros) {
        Tensor::logical_not(mask,mask);

        Tensor *A=replicate_tensor(mask,units);

        Tensor::el_mult(A,state_h,state_h,0);
        Tensor::el_mult(A,state_c,state_c,0);

        delete A;

        if (parent.size()>1) {
            Tensor::inc(psh,state_h); //output=prev output when in=0
            Tensor::inc(psc,state_c);

            delete psh;
            delete psc;
        }
    }

    if (!mode) { // eval mode
        delete in;
        delete fn;
        delete cn;
        delete on;
        delete incn;
        delete cn1fn;
        delete sh;
        if (mask_zeros) delete mask;
    }


}

void LLSTM::backward() {
    //delta_h=delta;
    //delta_c
    if (mask_zeros) {
        if (parent.size()>1) {
            Tensor::logical_not(mask,mask);

            Tensor *A=replicate_tensor(mask,units);

            psh=delta_h->clone();
            psc=delta_c->clone();

            Tensor::el_mult(A,psh,psh,0);
            Tensor::el_mult(A,psc,psc,0);

            delete A;

        }
    }

    Tensor *d1=new Tensor(delta->getShape(),dev);
    Tensor *d2=new Tensor(delta->getShape(),dev);
    Tensor *daux=new Tensor(delta->getShape(),dev);

    Tensor::el_mult(delta,on,d1,0);
    Tensor::el_mult(delta,sh,d2,0);

    // output gate
    daux->fill_(0.0);
    tensorNN::D_Sigmoid(d2, on, daux);
    Tensor::copy(daux,d2);

    if (trainable) {
        Tensor::mult2D(parent[0]->output, 1, d2, 0, gWox, 1);
        if (parent.size()>1)
            Tensor::mult2D(parent[1]->states[0], 1, d2, 0, gWoh, 1);
        Tensor::reduce_sum2D(d2, gonbias, 0, 1);
    }

    Tensor::mult2D(d2, 0, Wox, 1, parent[0]->delta, 1);
    if (parent.size()>1)
        Tensor::mult2D(d2, 0, Woh, 1, parent[1]->delta_states[0], 1);


    d2->fill_(0.0);
    tensorNN::D_Tanh(d1, sh, d2);
    Tensor::inc(d2,delta_c);

    // forget gate
    if (parent.size()>1) {
        Tensor::el_mult(delta_c, fn, parent[1]->delta_states[1], 1);
        Tensor::el_mult(delta_c, parent[1]->states[1], d2, 0);

        daux->fill_(0.0);
        tensorNN::D_Sigmoid(d2, fn, daux);
        Tensor::copy(daux,d2);

        if (trainable) {
            Tensor::mult2D(parent[0]->output, 1, d2, 0, gWfx, 1);
            Tensor::mult2D(parent[1]->states[0], 1, d2, 0, gWfh, 1);
            Tensor::reduce_sum2D(d2, gfnbias, 0, 1);
        }
        Tensor::mult2D(d2, 0, Wfx, 1, parent[0]->delta, 1);
        Tensor::mult2D(d2, 0, Wfh, 1, parent[1]->delta_states[0], 1);
    }

    Tensor::el_mult(delta_c, in, d1, 0);
    Tensor::el_mult(delta_c, cn, d2, 0);

    // Input gate
    daux->fill_(0.0);
    tensorNN::D_Sigmoid(d2, in, daux);
    Tensor::copy(daux,d2);

    if (trainable) {
        Tensor::mult2D(parent[0]->output, 1, d2, 0, gWix, 1);
        if (parent.size()>1)
            Tensor::mult2D(parent[1]->states[0], 1, d2, 0, gWih, 1);
        Tensor::reduce_sum2D(d2, ginbias, 0, 1);
    }

    Tensor::mult2D(d2, 0, Wix, 1, parent[0]->delta, 1);
    if (parent.size()>1)
        Tensor::mult2D(d2, 0, Wih, 1, parent[1]->delta_states[0], 1);

    // Cn
    daux->fill_(0.0);
    tensorNN::D_Tanh(d1, cn, daux);
    Tensor::copy(daux,d1);

    if (trainable) {
        Tensor::mult2D(parent[0]->output, 1, d1, 0, gWcx, 1);
        if (parent.size()>1)
            Tensor::mult2D(parent[1]->states[0], 1, d1, 0, gWch, 1);
        Tensor::reduce_sum2D(d1, gcnbias, 0, 1);
    }

    Tensor::mult2D(d1, 0, Wcx, 1, parent[0]->delta, 1);
    if (parent.size()>1)
        Tensor::mult2D(d1, 0, Wch, 1, parent[1]->delta_states[0], 1);


    if (mask_zeros) {
        if (parent.size()>1) {
            Tensor::logical_not(mask,mask);

            Tensor *A=replicate_tensor(mask,units);

            Tensor::el_mult(A,parent[1]->delta_states[0],parent[1]->delta_states[0],0);
            Tensor::el_mult(A,parent[1]->delta_states[1],parent[1]->delta_states[1],0);
            delete A;

            Tensor::inc(psh,parent[1]->delta_states[0]);
            Tensor::inc(psc,parent[1]->delta_states[1]);

            delete psh;
            delete psc;

        }
        delete mask;
    }


    delete d1;
    delete d2;
    delete daux;
    delete in;
    delete fn;
    delete cn;
    delete on;
    delete incn;
    delete cn1fn;
    delete sh;

}


Layer *LLSTM::share(int c, int bs, vector<Layer *> p) {
    LLSTM *n = new LLSTM(p, units, mask_zeros, bidirectional, "share_"+to_string(c)+this->name, this->dev, this->mem_level);
    n->orig = this;
    n->isshared = true;
    n->do_deletes = false;

    //share params
    for (int i = 0; i < n->params.size(); i++) delete n->params[i];
    n->params.clear();

    n->Wox = Wox;
    n->Wix = Wix;
    n->Wfx = Wfx;
    n->Wcx = Wcx;
    n->params.push_back(Wix);
    n->params.push_back(Wfx);
    n->params.push_back(Wox);
    n->params.push_back(Wcx);
    n->inbias = inbias;
    n->fnbias = fnbias;
    n->onbias = onbias;
    n->cnbias = cnbias;
    n->params.push_back(inbias);
    n->params.push_back(fnbias);
    n->params.push_back(onbias);
    n->params.push_back(cnbias);

    if (n->parent.size()>1) {
        n->Woh = Woh;
        n->Wih = Wih;
        n->Wfh = Wfh;
        n->Wch = Wch;
        n->params.push_back(Wih);
        n->params.push_back(Wfh);
        n->params.push_back(Woh);
        n->params.push_back(Wch);
    }


    //share gradients
    for (int i = 0; i < n->gradients.size(); i++) delete n->gradients[i];
    n->gradients.clear();

    n->gWox = gWox;
    n->gWix = gWix;
    n->gWfx = gWfx;
    n->gWcx = gWcx;
    n->gradients.push_back(gWix);
    n->gradients.push_back(gWfx);
    n->gradients.push_back(gWox);
    n->gradients.push_back(gWcx);
    n->ginbias = ginbias;
    n->gfnbias = gfnbias;
    n->gonbias = gonbias;
    n->gcnbias = gcnbias;
    n->gradients.push_back(ginbias);
    n->gradients.push_back(gfnbias);
    n->gradients.push_back(gonbias);
    n->gradients.push_back(gcnbias);
    if (n->parent.size()>1) {
        n->gWoh = gWoh;
        n->gWih = gWih;
        n->gWfh = gWfh;
        n->gWch = gWch;
        n->gradients.push_back(gWih);
        n->gradients.push_back(gWfh);
        n->gradients.push_back(gWoh);
        n->gradients.push_back(gWch);
    }


    if (n->reg != nullptr) delete n->reg;
    n->reg = reg;
    if (n->init != nullptr) delete n->init;
    n->init = init;

    return n;
}

Layer *LLSTM::clone(int c, int bs, vector<Layer *> p, int todev) {
    LLSTM *n = new LLSTM(p, units, mask_zeros, bidirectional,  name, todev, this->mem_level);
    n->orig = this;

    // TODO: Implement

    return n;
}


string LLSTM::plot(int c) {
    string s;

    if (c) s = name + " [label=" + "\"" + name + "\",style=filled,fontsize=12,fillcolor=bisque4,shape=box]";
    else s = name + " [label=" + "\"" + name + "\",style=filled,fontsize=12,fillcolor=White,shape=box]";

    return s;
}
