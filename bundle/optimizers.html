<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimizers &mdash; EDDL  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CPU" href="computing_service.html" />
    <link rel="prev" title="Initializers" href="initializers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #185070" >
            <a href="../index.html">
            <img src="../_static/logo-eddl-small-white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/build-options.html">Build and configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/troubleshoot.html">Troubleshoot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../usage/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/intermediate.html">Intermediate models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/advanced.html">Advanced models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/pretrained.html">Pretrained models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Videotutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../videotutorials/developers.html">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../videotutorials/usage.html">Showcase</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Layers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../layers/core.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/auxiliar.html">Auxiliar Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/data_augmentation.html">Data augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/data_transformation.html">Data transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/convolutional.html">Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/noise.html">Noise Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/pooling.html">Pooling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/normalization.html">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/merge.html">Merge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/generators.html">Generators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/operators.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/reduction.html">Reduce</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers/rnn.html">Recurrent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/onnx.html">Save and load ONNX models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../training/coarse.html">Coarse training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/fine_grained.html">Fine-grained training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Test &amp; Score</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../test_score/test_score.html">Test &amp; Score</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Bundle</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="regularizers.html">Regularizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializers.html">Initializers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#adadelta">Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adam">Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adagrad">Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adamax">Adamax</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nadam">Nadam</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rmsprop">RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#export-to-file">Export to file</a></li>
<li class="toctree-l2"><a class="reference internal" href="#import-from-file">Import from file</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Computing Services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="computing_service.html">CPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="computing_service.html#gpu">GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="computing_service.html#fpga">FPGA</a></li>
<li class="toctree-l1"><a class="reference internal" href="computing_service.html#compss">COMPSS</a></li>
<li class="toctree-l1"><a class="reference internal" href="computing_service.html#serialization">Serialization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Zoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../models_zoo/classification.html">Classification</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../datasets/classification.html">Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets/segmentation.html">Segmentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tensor</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tensor/create.html">Creation Routines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/manipulation.html">Manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/image.html">Image operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/indexing.html">Indexing &amp; Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/input_output.html">Input/Output Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/linear_algebra.html">Linear algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/logic_functions.html">Logic functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/math.html">Mathematical functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor/misc.html">Misc</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #185070" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">EDDL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Optimizers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/bundle/optimizers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="optimizers">
<h1>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline"></a></h1>
<div class="section" id="adadelta">
<h2>Adadelta<a class="headerlink" href="#adadelta" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl8adadeltaEffff">
<span id="_CPPv3N4eddl8adadeltaEffff"></span><span id="_CPPv2N4eddl8adadeltaEffff"></span><span id="eddl::adadelta__float.float.float.float"></span><span class="target" id="namespaceeddl_1ab7990db93dd516ae3ad707f757d02296"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">adadelta</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">rho</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">epsilon</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">weight_decay</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl8adadeltaEffff" title="Permalink to this definition"></a><br /></dt>
<dd><p>Adadelta optimizer. </p>
<p>Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done.</p>
<p><dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference external" href="https://arxiv.org/abs/1212.5701">https://arxiv.org/abs/1212.5701</a></p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>rho</strong> – Smoothing constant </p></li>
<li><p><strong>epsilon</strong> – Term added to the denominator to improve numerical stability </p></li>
<li><p><strong>weight_decay</strong> – Weight decay (L2 penalty) </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Adadelta optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">adadelta</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.000001</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="adam">
<h2>Adam<a class="headerlink" href="#adam" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl4adamEfffffb">
<span id="_CPPv3N4eddl4adamEfffffb"></span><span id="_CPPv2N4eddl4adamEfffffb"></span><span id="eddl::adam__float.float.float.float.float.b"></span><span class="target" id="namespaceeddl_1a1245376cb4f64d7addf20e8ef1b5e982"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">adam</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.01</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">beta_1</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.9</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">beta_2</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.999</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">epsilon</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.000001</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">weight_decay</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">amsgrad</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl4adamEfffffb" title="Permalink to this definition"></a><br /></dt>
<dd><p>Adam optimizer. </p>
<p>Default parameters follow those provided in the original paper (See section). <dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference external" href="https://arxiv.org/abs/1412.6980v8">https://arxiv.org/abs/1412.6980v8</a></p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>beta_1</strong> – Coefficients used for computing running averages of gradient and its square </p></li>
<li><p><strong>beta_2</strong> – Coefficients used for computing running averages of gradient and its square </p></li>
<li><p><strong>epsilon</strong> – Term added to the denominator to improve numerical stability </p></li>
<li><p><strong>weight_decay</strong> – Weight decay (L2 penalty) </p></li>
<li><p><strong>amsgrad</strong> – Whether to apply the AMSGrad variant of this algorithm from the paper “On the Convergence of Adam and Beyond”. </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Adam optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">adam</span><span class="p">(</span><span class="mf">0.001</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="adagrad">
<h2>Adagrad<a class="headerlink" href="#adagrad" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl7adagradEfff">
<span id="_CPPv3N4eddl7adagradEfff"></span><span id="_CPPv2N4eddl7adagradEfff"></span><span id="eddl::adagrad__float.float.float"></span><span class="target" id="namespaceeddl_1afe8db87b88a3267f0be7e7496bc294f4"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">adagrad</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">epsilon</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">weight_decay</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl7adagradEfff" title="Permalink to this definition"></a><br /></dt>
<dd><p>Adagrad optimizer. </p>
<p>Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the learning rate.</p>
<p><dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference external" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>rho</strong> – Smoothing constant </p></li>
<li><p><strong>epsilon</strong> – Term added to the denominator to improve numerical stability </p></li>
<li><p><strong>weight_decay</strong> – Weight decay (L2 penalty) </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Adagrad optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">adagrad</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.000001</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="adamax">
<h2>Adamax<a class="headerlink" href="#adamax" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl6adamaxEfffff">
<span id="_CPPv3N4eddl6adamaxEfffff"></span><span id="_CPPv2N4eddl6adamaxEfffff"></span><span id="eddl::adamax__float.float.float.float.float"></span><span class="target" id="namespaceeddl_1a325ff1a6fdb5f7065b806695ded3f693"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">adamax</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">beta_1</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">beta_2</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">epsilon</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">weight_decay</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl6adamaxEfffff" title="Permalink to this definition"></a><br /></dt>
<dd><p>Adamax optimizer. </p>
<p>It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the See section. <dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference external" href="https://arxiv.org/abs/1412.6980v8">https://arxiv.org/abs/1412.6980v8</a></p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>beta_1</strong> – Coefficients used for computing running averages of gradient and its square </p></li>
<li><p><strong>beta_2</strong> – Coefficients used for computing running averages of gradient and its square </p></li>
<li><p><strong>epsilon</strong> – Term added to the denominator to improve numerical stability </p></li>
<li><p><strong>weight_decay</strong> – Weight decay (L2 penalty) </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Adamax optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">adamax</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">,</span><span class="w"> </span><span class="mf">0.000001</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="nadam">
<h2>Nadam<a class="headerlink" href="#nadam" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl5nadamEfffff">
<span id="_CPPv3N4eddl5nadamEfffff"></span><span id="_CPPv2N4eddl5nadamEfffff"></span><span id="eddl::nadam__float.float.float.float.float"></span><span class="target" id="namespaceeddl_1a91dbb377770ae4cb5361ad824149b13a"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">nadam</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">beta_1</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">beta_2</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">epsilon</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">schedule_decay</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl5nadamEfffff" title="Permalink to this definition"></a><br /></dt>
<dd><p>Nadam optimizer. </p>
<p>It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the See section. <dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference external" href="https://arxiv.org/abs/1412.6980v8">https://arxiv.org/abs/1412.6980v8</a></p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>beta_1</strong> – Coefficients used for computing running averages of gradient and its square </p></li>
<li><p><strong>beta_2</strong> – Coefficients used for computing running averages of gradient and its square </p></li>
<li><p><strong>epsilon</strong> – Term added to the denominator to improve numerical stability </p></li>
<li><p><strong>schedule_decay</strong> – Weight decay (L2 penalty) </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Nadam optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nadam</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0000001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.004</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="rmsprop">
<h2>RMSProp<a class="headerlink" href="#rmsprop" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl7rmspropEffff">
<span id="_CPPv3N4eddl7rmspropEffff"></span><span id="_CPPv2N4eddl7rmspropEffff"></span><span id="eddl::rmsprop__float.float.float.float"></span><span class="target" id="namespaceeddl_1a8109148bc89e06fe446881e5c61c7a03"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">rmsprop</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.01</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">rho</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.9</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">epsilon</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.00001</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">weight_decay</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl7rmspropEffff" title="Permalink to this definition"></a><br /></dt>
<dd><p>RMSProp optimizer. </p>
<p>It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned).</p>
<p><dl class="simple">
<dt><strong>See</strong></dt><dd><p><a class="reference external" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>rho</strong> – Smoothing constant </p></li>
<li><p><strong>epsilon</strong> – Term added to the denominator to improve numerical stability </p></li>
<li><p><strong>weight_decay</strong> – Weight decay (L2 penalty) </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>RMSProp optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rmsprop</span><span class="p">(</span><span class="mf">0.001</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="sgd-stochastic-gradient-descent">
<h2>SGD (Stochastic Gradient Descent)<a class="headerlink" href="#sgd-stochastic-gradient-descent" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N4eddl3sgdEfffb">
<span id="_CPPv3N4eddl3sgdEfffb"></span><span id="_CPPv2N4eddl3sgdEfffb"></span><span id="eddl::sgd__float.float.float.b"></span><span class="target" id="namespaceeddl_1ae030ab89d6778ba90ca5d5e0e8587c41"></span><span class="n"><span class="pre">optimizer</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">eddl</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">sgd</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">lr</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.01f</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">momentum</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.0f</span></span>, <span class="kt"><span class="pre">float</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">weight_decay</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.0f</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">nesterov</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N4eddl3sgdEfffb" title="Permalink to this definition"></a><br /></dt>
<dd><p>Stochastic gradient descent optimizer. </p>
<p>Includes support for momentum, learning rate decay, and Nesterov momentum</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> – Learning rate </p></li>
<li><p><strong>momentum</strong> – Momentum factor </p></li>
<li><p><strong>weight_decay</strong> – Value to apply to the activation function </p></li>
<li><p><strong>nesterov</strong> – Boolean. Whether to apply Nesterov momentum </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Stochastic gradient descent optimizer </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sgd</span><span class="p">(</span><span class="mf">0.001</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="export-to-file">
<h2>Export to file<a class="headerlink" href="#export-to-file" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv427save_optimizer_to_onnx_fileP9Optimizer6string">
<span id="_CPPv327save_optimizer_to_onnx_fileP9Optimizer6string"></span><span id="_CPPv227save_optimizer_to_onnx_fileP9Optimizer6string"></span><span id="save_optimizer_to_onnx_file__OptimizerP.string"></span><span class="target" id="eddl__onnx_8h_1a8c35a77229eed96288531b7c0746b102"></span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">save_optimizer_to_onnx_file</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">Optimizer</span></span><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="n sig-param"><span class="pre">optimizer</span></span>, <span class="n"><span class="pre">string</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">path</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv427save_optimizer_to_onnx_fileP9Optimizer6string" title="Permalink to this definition"></a><br /></dt>
<dd><p>Saves the configuration of an optimizer using the ONNX format. It will contain the Optimizer type and attributes like learning rate, momentum, weight decay, etc. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> – Optimizer to be saved </p></li>
<li><p><strong>path</strong> – Path to the file where the Optimizer configuration will be saved</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(void) </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span><span class="w"> </span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sgd</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9</span><span class="p">);</span>
<span class="n">save_optimizer_to_onnx_file</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;my_opt.onnx&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="import-from-file">
<h2>Import from file<a class="headerlink" href="#import-from-file" title="Permalink to this headline"></a></h2>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv431import_optimizer_from_onnx_file6string">
<span id="_CPPv331import_optimizer_from_onnx_file6string"></span><span id="_CPPv231import_optimizer_from_onnx_file6string"></span><span id="import_optimizer_from_onnx_file__string"></span><span class="target" id="eddl__onnx_8h_1a9b58d4b7d4c1a4974602ca21659d3f63"></span><span class="n"><span class="pre">Optimizer</span></span><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="sig-name descname"><span class="n"><span class="pre">import_optimizer_from_onnx_file</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">string</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">path</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv431import_optimizer_from_onnx_file6string" title="Permalink to this definition"></a><br /></dt>
<dd><p>Creates an Optimizer from the definition provided in an ONNX file. The ONNX will provide the Optimizer type and its attributes. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> – Path to the file where the Optimizer configuration is saved</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Optimizer* </p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span><span class="w"> </span><span class="n">opt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">import_optimizer_from_onnx_file</span><span class="p">(</span><span class="s">&quot;my_opt.onnx&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="initializers.html" class="btn btn-neutral float-left" title="Initializers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="computing_service.html" class="btn btn-neutral float-right" title="CPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, EDDL.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>